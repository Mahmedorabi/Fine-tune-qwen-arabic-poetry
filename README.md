# Arabic Poetry AI â€” Fine-tuning Qwen3-1.7B with LoRA

This project demonstrates how to fine-tune the [`Qwen/Qwen3-1.7B`](https://huggingface.co/Qwen/Qwen3-1.7B) model on a synthetic Arabic poetry dataset using **LoRA (Low-Rank Adaptation)**. The result is an efficient Arabic-language model capable of generating poetic responses to user prompts.

---

## ğŸš€ Project Highlights

* ğŸ”¤ Language: **Arabic**
* ğŸ§  Base Model: [`Qwen/Qwen3-1.7B`](https://huggingface.co/Qwen/Qwen3-1.7B)
* ğŸ§© Fine-tuning Technique: **LoRA (r=8, alpha=16)**
* â±ï¸ Training Time: \~24 minutes on Google Colab L4 GPU
* ğŸ“¦ Output Model: [Hugging Face Repo](https://huggingface.co/mohammed-orabi2/qwen-poetry-arabic-lora)

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ arabic_poetry_1000.json       # Synthetic Arabic poetry dataset
â”œâ”€â”€ requirements.txt              # Project dependencies
â”œâ”€â”€ train_lora.py                 # Full training & deployment script
â””â”€â”€ README.md                     # You're here
```

---

## ğŸ›  Requirements

Install dependencies with:

```bash
pip install -r requirements.txt
```

---

## ğŸ“š Dataset

The dataset consists of 1000 examples of user prompts in Arabic with corresponding poetic responses. All formatted in conversational style (role: user/assistant) to match chat model expectations.

---

## ğŸ§ª Training Workflow

```python
# 1. Load & format data
# 2. Apply chat template using Qwen tokenizer
# 3. Tokenize, pad, and set labels for CausalLM
# 4. Apply LoRA (q_proj, v_proj)
# 5. Train for 5 epochs using Hugging Face Trainer
```

Key libraries used:

* `transformers`
* `datasets`
* `peft`
* `huggingface_hub`

---

## ğŸ§¾ Example Inference

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-1.7B")
model = PeftModel.from_pretrained(base, "mohammed-orabi2/qwen-poetry-lora2")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-1.7B")

prompt = "Ø§ÙƒØªØ¨ Ù„ÙŠ Ø¨ÙŠØª Ø´Ø¹Ø± Ø¹Ù† Ø§Ù„Ù†Ø¬Ø§Ø­."
chat = [{"role": "user", "content": prompt}]
formatted = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(formatted, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## â˜ï¸ Deploy to Hugging Face

```python
from huggingface_hub import HfApi
api = HfApi()
api.upload_folder(
    folder_path="qwen-poetry-lora2",
    repo_id="mohammed-orabi2/qwen-poetry-lora2",
    repo_type="model"
)
```

---

## ğŸ“ˆ Results

* 90% of generated responses followed poetic structure and matched topics.
* Excellent performance for creative and expressive Arabic content.

---

## ğŸ“Œ Notes

* The model is not intended for factual tasks or safety-critical applications.
* Designed specifically for educational and creative purposes.

---

## ğŸ‘¤ Author

**Mohammed Orabi**
[Hugging Face](https://huggingface.co/mohammed-orabi2)

---

## ğŸ“„ License

Apache 2.0 (inherited from Qwen3-1.7B)

---

## ğŸ’¬ Feedback & Contributions

Pull requests and issues are welcome! Letâ€™s collaborate to expand Arabic LLM applications.
